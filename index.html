<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-How-to-Computing-the-PSNR" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/06/26/How-to-Computing-the-PSNR/" class="article-date">
  <time datetime="2019-06-26T09:10:38.000Z" itemprop="datePublished">2019-06-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/06/26/How-to-Computing-the-PSNR/">How to Computing the PSNR</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/06/26/How-to-Computing-the-PSNR/" data-id="cjxd53jlj0001i8v7o92l1jir" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-批归一化Batch-Normalization的原理及算法" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/06/25/批归一化Batch-Normalization的原理及算法/" class="article-date">
  <time datetime="2019-06-25T14:29:41.000Z" itemprop="datePublished">2019-06-25</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/06/25/批归一化Batch-Normalization的原理及算法/">批归一化Batch Normalization的原理及算法</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>#一、BN提出的背景意义<br>本文主体背景是：《Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift》，Batch Normalization 算法目前已经被大量 的应用，最新的文献算法很多都会引用这个算法，进行网络训练，可见其强大之处。</p>
<p>随机梯度下降法是训练深度网络的首选。尽管随机梯度下降法对于训练深度网络简单高效，但是需要我们人为的去选择参数，比如学习速率、初始化参数、权重衰减系数、Dropout比例，等等。这些参数的选择对训练结果至关重要，以至于我们很多时间都浪费在这些的调参上。BN算法（Batch Normalization）其好处如下：</p>
<p>##1.可以选择比较大的初始学习率，极大的提高训练速度。<br>Batch Gradient Descent使用多个梯度的均值来更新权重，用相对少的训练次数遍历完整个训练集，也正是因为平均了多个样本的梯度，许多样本对神经网络的贡献就被其他样本平均掉了，相当于在每个epoch中，训练集的样本数被缩小了。batch中每个样本的差异性越大，这种弊端就越严重。BN首先是把所有的samples的统计分布标准化，降低了batch内不同样本的差异性，然后又允许batch内的各个samples有各自的统计分布。所以，BN的优点自然也就是允许网络使用较大的学习速率进行训练，加快网络的训练速度（减少epoch次数），提升效果。</p>
<p>##2.省去参数选择的问题。<br>省去过拟合中drop out、L2正则项参数的选择问题，采用BN算法后，可以移除这两项了参数，或者可以选择更小的L2正则约束参数了，因为BN具有提高网络泛化能力的特性；</p>
<p>##为什么需要归一化</p>
<p>神经网络学习过程本质上就是为了学习数据分布，一旦训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低。</p>
<p>输入层的数据，已经认为的归一化，后面网络每一层的输入数据分布是一直在发生变化的，前面层训练参数的更新将导致后面层输入数据分布的变化，因此必然会引起后面每一层输入数据分布的改变。而且，网络的前面几层发生微小的改变，那么后面几层就会被累积放大下去。我们把网络中间层在训练过程中，数据分布的改变称之为：”Internal Covariate Shift”。BN的提出，就是要解决在训练过程中，中间层数据分布发生改变的情况。</p>
<p>#二。BN算法原理</p>
<p>##1、BN层及使用位置<br>就像激活函数层、卷积层、全连接层、池化层一样，BN(Batch Normalization)也属于网络的一层。归一化也是网络的一层。</p>
<p>在网络的每一层输入的时候，又插入了一个归一化层，也就是先做一个归一化处理，然后再进入网络的下一层。如果在每一层输入的时候，再加个预处理操作那该有多好啊，比如网络第三层输入数据X3(X3表示网络第三层的输入数据)把它归一化至：均值0、方差为1，然后再输入第三层计算，这样我们就可以解决前面所提到的”Internal Covariate Shift”的问题了。</p>
<p>关于BN的使用位置，在CNN中一般应作用与非线性激活函数之前，s型函数s(x)的自变量x是经过BN处理后的结果。因此前向传导的计算公式就应该是：</p>
<p><img src="/images/BN_fomula1.png" alt></p>
<p>其实因为偏置参数b经过BN层后其实是没有用的，最后也会被均值归一化，当然BN层后面还有个β参数作为偏置项，所以b这个参数就可以不用了。因此最后把BN层+激活函数层就变成了：</p>
<p><img src="/images/BN_fomula2.png" alt></p>
<p>##2、预处理操作选择</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/06/25/批归一化Batch-Normalization的原理及算法/" data-id="cjxd53jlo0003i8v7ab5asrly" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-opencv-study" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/05/29/opencv-study/" class="article-date">
  <time datetime="2019-05-29T08:38:23.000Z" itemprop="datePublished">2019-05-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/29/opencv-study/">opencv-study</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>#</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/05/29/opencv-study/" data-id="cjxd53jld0000i8v79mt27wt4" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-ONNX-Learning" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/03/25/ONNX-Learning/" class="article-date">
  <time datetime="2019-03-25T04:28:19.000Z" itemprop="datePublished">2019-03-25</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/03/25/ONNX-Learning/">ONNX Learning</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/03/25/ONNX-Learning/" data-id="cjxd53jlm0002i8v7n0t7zw5o" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">June 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/06/26/How-to-Computing-the-PSNR/">How to Computing the PSNR</a>
          </li>
        
          <li>
            <a href="/2019/06/25/批归一化Batch-Normalization的原理及算法/">批归一化Batch Normalization的原理及算法</a>
          </li>
        
          <li>
            <a href="/2019/05/29/opencv-study/">opencv-study</a>
          </li>
        
          <li>
            <a href="/2019/03/25/ONNX-Learning/">ONNX Learning</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>